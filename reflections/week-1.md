# Week 1 Reflection – Learning Coach Experiment

## What I Tried
I designed an AI assistant framed as a “learning coach” rather than an answer generator.
The goal was to observe how role framing influences reasoning depth and engagement.

## What Worked
- The coach framing produced more structured, step-by-step responses.
- Asking clarifying questions first changed the tone from authoritative to collaborative.
- The workflow made implicit assumptions visible.

## What Didn’t
- Over-constraining the role risks slowing down users who want quick answers.
- Some prompts needed clearer boundaries to avoid verbosity.

## What Surprised Me
Small wording changes in the role description led to noticeable differences in behavior.
This reinforced how sensitive AI systems are to framing and constraints.

## What I’ll Do Next
- Compare coach vs tutor vs peer roles.
- Introduce adaptive depth based on user signals.
- Track failure cases more explicitly.

## Meta
I’m intentionally documenting incomplete thoughts.
Clarity emerges through iteration, not perfection.
